# DD Owl Auto-Mode Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build auto-mode that crawls QCC profiles with pagination, stores data in SQLite, and generates DD reports.

**Architecture:** Extension extracts DOM → WebSocket sends to backend → Backend stores in SQLite and queues linked URLs → Puppeteer navigates to next URL → repeat until queue empty → generate Word report.

**Tech Stack:** Chrome Extension (Manifest V3), TypeScript backend, SQLite, Puppeteer, WebSocket, docx library

---

## Task 1: Set Up SQLite Database

**Files:**
- Create: `ddowl/src/database.ts`
- Modify: `ddowl/package.json` (add better-sqlite3)

**Step 1: Install SQLite dependency**

```bash
cd "/Users/home/Desktop/DD Owl/ddowl"
npm install better-sqlite3
npm install -D @types/better-sqlite3
```

**Step 2: Create database module**

```typescript
// ddowl/src/database.ts
import Database from 'better-sqlite3';
import path from 'path';

const db = new Database(path.join(__dirname, '../data/ddowl.db'));

// Initialize tables
db.exec(`
  CREATE TABLE IF NOT EXISTS persons (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    qcc_url TEXT UNIQUE,
    extracted_at TEXT,
    raw_json TEXT
  );

  CREATE TABLE IF NOT EXISTS companies (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    qcc_url TEXT UNIQUE,
    unified_credit_code TEXT,
    legal_representative TEXT,
    registered_capital TEXT,
    established_date TEXT,
    operating_status TEXT,
    extracted_at TEXT,
    raw_json TEXT
  );

  CREATE TABLE IF NOT EXISTS affiliations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    person_id INTEGER,
    company_id INTEGER,
    position TEXT,
    appointment_date TEXT,
    resignation_date TEXT,
    status TEXT,
    FOREIGN KEY (person_id) REFERENCES persons(id),
    FOREIGN KEY (company_id) REFERENCES companies(id)
  );

  CREATE TABLE IF NOT EXISTS crawl_queue (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    url TEXT UNIQUE,
    type TEXT CHECK(type IN ('person', 'company')),
    status TEXT DEFAULT 'pending' CHECK(status IN ('pending', 'processing', 'completed', 'failed')),
    parent_url TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP
  );
`);

export function savePerson(data: any) {
  const stmt = db.prepare(`
    INSERT OR REPLACE INTO persons (name, qcc_url, extracted_at, raw_json)
    VALUES (?, ?, ?, ?)
  `);
  return stmt.run(data.personName, data.sourceUrl, data.extractedAt, JSON.stringify(data));
}

export function saveCompany(data: any) {
  const stmt = db.prepare(`
    INSERT OR REPLACE INTO companies (name, qcc_url, unified_credit_code, legal_representative,
      registered_capital, established_date, operating_status, extracted_at, raw_json)
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
  `);
  return stmt.run(
    data.companyName,
    data.sourceUrl,
    data.unifiedSocialCreditCode,
    data.legalRepresentative,
    data.registeredCapital,
    data.establishedDate,
    data.operatingStatus,
    data.extractedAt,
    JSON.stringify(data)
  );
}

export function queueUrl(url: string, type: 'person' | 'company', parentUrl?: string) {
  const stmt = db.prepare(`
    INSERT OR IGNORE INTO crawl_queue (url, type, parent_url)
    VALUES (?, ?, ?)
  `);
  return stmt.run(url, type, parentUrl);
}

export function getNextInQueue() {
  const stmt = db.prepare(`
    SELECT * FROM crawl_queue WHERE status = 'pending' ORDER BY id LIMIT 1
  `);
  return stmt.get();
}

export function markQueueItem(id: number, status: 'processing' | 'completed' | 'failed') {
  const stmt = db.prepare(`UPDATE crawl_queue SET status = ? WHERE id = ?`);
  return stmt.run(status, id);
}

export function getQueueStats() {
  const stmt = db.prepare(`
    SELECT status, COUNT(*) as count FROM crawl_queue GROUP BY status
  `);
  return stmt.all();
}

export default db;
```

**Step 3: Create data directory**

```bash
mkdir -p "/Users/home/Desktop/DD Owl/ddowl/data"
```

**Step 4: Verify it compiles**

```bash
cd "/Users/home/Desktop/DD Owl/ddowl"
npx tsc --noEmit
```

---

## Task 2: Integrate Database with Server

**Files:**
- Modify: `ddowl/src/server.ts`

**Step 1: Import database and save extracted data**

Add to server.ts after receiving EXTRACTED_DATA:

```typescript
import { savePerson, saveCompany, queueUrl, getQueueStats } from './database';

// In WebSocket message handler for EXTRACTED_DATA:
if (message.type === 'EXTRACTED_DATA') {
  const data = message.data;

  if (data.pageType === 'person_profile') {
    savePerson(data);
    // Queue all affiliated companies
    data.companies?.forEach((c: any) => {
      if (c.profileUrl) {
        queueUrl(c.profileUrl, 'company', data.sourceUrl);
      }
    });
  } else if (data.pageType === 'company_profile') {
    saveCompany(data);
    // Queue linked profiles
    data.linkedProfiles?.forEach((p: any) => {
      if (p.url) {
        queueUrl(p.url, p.type, data.sourceUrl);
      }
    });
  }

  // Send acknowledgment with queue stats
  ws.send(JSON.stringify({
    type: 'EXTRACTION_ACK',
    data: {
      message: `Saved ${data.pageType}`,
      queueStats: getQueueStats()
    }
  }));
}
```

**Step 2: Add status endpoint**

```typescript
// Add REST endpoint for queue status
app.get('/api/queue', (req, res) => {
  res.json(getQueueStats());
});
```

---

## Task 3: Add Pagination Detection to Content Script

**Files:**
- Modify: `owl1-extension/content-scripts/qcc.js`

**Step 1: Add pagination detection function**

Add before the message listener:

```javascript
// Detect pagination on page
function detectPagination() {
  const pagination = {
    hasPagination: false,
    currentPage: 1,
    totalPages: 1,
    nextPageUrl: null,
    pageUrls: []
  };

  // QCC uses .ant-pagination or similar
  const paginationEl = document.querySelector('.ant-pagination, [class*="pagination"], .page-list');
  if (!paginationEl) return pagination;

  pagination.hasPagination = true;

  // Find current page
  const activePage = paginationEl.querySelector('.ant-pagination-item-active, [class*="active"], .current');
  if (activePage) {
    pagination.currentPage = parseInt(activePage.textContent) || 1;
  }

  // Find all page links
  const pageItems = paginationEl.querySelectorAll('.ant-pagination-item, [class*="page-item"]:not(.prev):not(.next)');
  pageItems.forEach(item => {
    const pageNum = parseInt(item.textContent);
    if (pageNum && !isNaN(pageNum)) {
      pagination.pageUrls.push({
        page: pageNum,
        element: item.tagName
      });
      if (pageNum > pagination.totalPages) {
        pagination.totalPages = pageNum;
      }
    }
  });

  // Find next button
  const nextBtn = paginationEl.querySelector('.ant-pagination-next:not(.ant-pagination-disabled), [class*="next"]:not(.disabled)');
  if (nextBtn) {
    pagination.nextPageUrl = 'HAS_NEXT';
  }

  return pagination;
}
```

**Step 2: Include pagination in extraction response**

Modify each extraction function to include pagination. At the end of `extractCompanyProfile()`:

```javascript
// 7. Detect pagination
data.pagination = detectPagination();

return data;
```

Same for `extractPersonProfile()` and `extractSearchResults()`.

---

## Task 4: Add Click-Next-Page Message Handler

**Files:**
- Modify: `owl1-extension/content-scripts/qcc.js`

**Step 1: Add CLICK_NEXT_PAGE handler**

In the message listener, add:

```javascript
if (message.type === 'CLICK_NEXT_PAGE') {
  try {
    const paginationEl = document.querySelector('.ant-pagination, [class*="pagination"]');
    if (!paginationEl) {
      sendResponse({ success: false, error: 'No pagination found' });
      return true;
    }

    const nextBtn = paginationEl.querySelector('.ant-pagination-next:not(.ant-pagination-disabled), [class*="next"]:not(.disabled)');
    if (!nextBtn) {
      sendResponse({ success: false, error: 'No next page available' });
      return true;
    }

    nextBtn.click();

    // Wait for content to load
    setTimeout(() => {
      sendResponse({ success: true, message: 'Clicked next page' });
    }, 1500);

    return true;
  } catch (error) {
    sendResponse({ success: false, error: error.message });
    return true;
  }
}

if (message.type === 'CLICK_PAGE') {
  try {
    const pageNum = message.pageNumber;
    const paginationEl = document.querySelector('.ant-pagination, [class*="pagination"]');
    if (!paginationEl) {
      sendResponse({ success: false, error: 'No pagination found' });
      return true;
    }

    const pageItems = paginationEl.querySelectorAll('.ant-pagination-item, [class*="page-item"]');
    let clicked = false;
    pageItems.forEach(item => {
      if (parseInt(item.textContent) === pageNum) {
        item.click();
        clicked = true;
      }
    });

    if (!clicked) {
      sendResponse({ success: false, error: `Page ${pageNum} not found` });
      return true;
    }

    setTimeout(() => {
      sendResponse({ success: true, message: `Clicked page ${pageNum}` });
    }, 1500);

    return true;
  } catch (error) {
    sendResponse({ success: false, error: error.message });
    return true;
  }
}
```

---

## Task 5: Create Puppeteer Crawler Module

**Files:**
- Create: `ddowl/src/crawler.ts`
- Modify: `ddowl/package.json` (add puppeteer-core)

**Step 1: Install puppeteer-core**

```bash
cd "/Users/home/Desktop/DD Owl/ddowl"
npm install puppeteer-core
```

**Step 2: Create crawler module**

```typescript
// ddowl/src/crawler.ts
import puppeteer, { Browser, Page } from 'puppeteer-core';
import { getNextInQueue, markQueueItem, queueUrl } from './database';

let browser: Browser | null = null;
let page: Page | null = null;
let isRunning = false;
let wsConnection: any = null;

export async function connectToChrome(debugPort: number = 9222) {
  try {
    browser = await puppeteer.connect({
      browserURL: `http://localhost:${debugPort}`,
      defaultViewport: null
    });

    const pages = await browser.pages();
    page = pages.find(p => p.url().includes('qcc.com')) || pages[0];

    console.log('Connected to Chrome, current page:', page?.url());
    return true;
  } catch (error) {
    console.error('Failed to connect to Chrome:', error);
    return false;
  }
}

export function setWsConnection(ws: any) {
  wsConnection = ws;
}

export async function startCrawling() {
  if (isRunning) {
    console.log('Crawler already running');
    return;
  }

  if (!browser || !page) {
    console.log('Not connected to Chrome');
    return;
  }

  isRunning = true;
  console.log('Starting crawl...');

  while (isRunning) {
    const nextItem = getNextInQueue();
    if (!nextItem) {
      console.log('Queue empty, stopping');
      break;
    }

    markQueueItem(nextItem.id, 'processing');

    try {
      // Navigate to URL
      await page.goto(nextItem.url, { waitUntil: 'networkidle2', timeout: 30000 });
      await page.waitForTimeout(2000); // Wait for dynamic content

      // Tell extension to extract
      if (wsConnection) {
        wsConnection.send(JSON.stringify({
          type: 'NAVIGATE_TO',
          data: { url: nextItem.url }
        }));
      }

      // Wait for extraction (extension will send data back)
      await page.waitForTimeout(3000);

      // Handle pagination - keep extracting until no more pages
      let hasNextPage = true;
      while (hasNextPage && isRunning) {
        // Check if there's pagination and more pages
        const pagination = await page.evaluate(() => {
          const nextBtn = document.querySelector('.ant-pagination-next:not(.ant-pagination-disabled)');
          return { hasNext: !!nextBtn };
        });

        if (pagination.hasNext) {
          await page.click('.ant-pagination-next');
          await page.waitForTimeout(2000);

          // Extract this page too
          if (wsConnection) {
            wsConnection.send(JSON.stringify({
              type: 'NAVIGATE_TO',
              data: { url: page.url() }
            }));
          }
          await page.waitForTimeout(3000);
        } else {
          hasNextPage = false;
        }
      }

      markQueueItem(nextItem.id, 'completed');

    } catch (error) {
      console.error('Error processing:', nextItem.url, error);
      markQueueItem(nextItem.id, 'failed');
    }

    // Small delay between pages
    await page.waitForTimeout(1000);
  }

  isRunning = false;
  console.log('Crawl finished');
}

export function stopCrawling() {
  isRunning = false;
}

export function getCrawlerStatus() {
  return {
    running: isRunning,
    connected: !!browser,
    currentUrl: page?.url()
  };
}
```

---

## Task 6: Wire Crawler to Server

**Files:**
- Modify: `ddowl/src/server.ts`

**Step 1: Import and expose crawler controls**

```typescript
import { connectToChrome, startCrawling, stopCrawling, getCrawlerStatus, setWsConnection } from './crawler';

// In WebSocket connection handler:
wss.on('connection', (ws) => {
  console.log('Extension connected');
  setWsConnection(ws);

  // ... existing message handling

  ws.on('message', (data) => {
    const message = JSON.parse(data.toString());

    // ... existing handlers

    if (message.type === 'START_AUTO_MODE') {
      connectToChrome().then(connected => {
        if (connected) {
          startCrawling();
          ws.send(JSON.stringify({ type: 'AUTO_MODE_STARTED' }));
        } else {
          ws.send(JSON.stringify({
            type: 'ERROR',
            data: 'Failed to connect to Chrome. Start Chrome with --remote-debugging-port=9222'
          }));
        }
      });
    }

    if (message.type === 'STOP_AUTO_MODE') {
      stopCrawling();
      ws.send(JSON.stringify({ type: 'AUTO_MODE_STOPPED' }));
    }

    if (message.type === 'GET_STATUS') {
      ws.send(JSON.stringify({
        type: 'STATUS_RESPONSE',
        data: {
          ...getCrawlerStatus(),
          queueStats: getQueueStats()
        }
      }));
    }
  });
});
```

---

## Task 7: Add Auto-Mode UI to Extension Popup

**Files:**
- Modify: `owl1-extension/popup/popup.html`
- Modify: `owl1-extension/popup/popup.js`

**Step 1: Update popup.html with queue display**

Add after the auto-status div:

```html
<!-- Queue Status -->
<div id="queueStatus" class="status-row" style="display: none;">
  <span class="status-label">Queue</span>
  <span class="status-value">
    <span id="queuePending">0</span> pending /
    <span id="queueCompleted">0</span> done
  </span>
</div>

<!-- Start Auto Button -->
<button id="startAutoBtn" class="secondary" style="display: none;">Start Auto Crawl</button>
```

**Step 2: Update popup.js to handle auto mode**

Add to popup.js:

```javascript
const startAutoBtn = document.getElementById('startAutoBtn');
const queueStatusEl = document.getElementById('queueStatus');

// Show auto controls when in auto mode
autoModeBtn.addEventListener('click', () => {
  currentMode = 'auto';
  autoModeBtn.classList.add('active');
  manualModeBtn.classList.remove('active');
  startAutoBtn.style.display = 'block';
  queueStatusEl.style.display = 'flex';
});

// Start auto crawl
startAutoBtn.addEventListener('click', () => {
  chrome.runtime.sendMessage({ type: 'START_AUTO_MODE' }, (response) => {
    if (response && response.success) {
      startAutoBtn.textContent = 'Running...';
      startAutoBtn.disabled = true;
    }
  });
});

// Update queue stats in status check
function updateStatus(response) {
  // ... existing status update code

  if (response && response.queueStats) {
    const stats = response.queueStats;
    const pending = stats.find(s => s.status === 'pending')?.count || 0;
    const completed = stats.find(s => s.status === 'completed')?.count || 0;
    document.getElementById('queuePending').textContent = pending;
    document.getElementById('queueCompleted').textContent = completed;
  }
}
```

---

## Task 8: Create Report Generator

**Files:**
- Create: `ddowl/src/report.ts`
- Modify: `ddowl/package.json` (add docx)

**Step 1: Install docx library**

```bash
cd "/Users/home/Desktop/DD Owl/ddowl"
npm install docx
```

**Step 2: Create report generator**

```typescript
// ddowl/src/report.ts
import { Document, Paragraph, TextRun, Table, TableRow, TableCell, HeadingLevel, Packer } from 'docx';
import fs from 'fs';
import db from './database';

interface PersonReport {
  name: string;
  affiliations: Array<{
    companyName: string;
    position: string;
    appointmentDate: string;
    status: string;
  }>;
}

export async function generatePersonReport(personUrl: string): Promise<Buffer> {
  // Get person data
  const person = db.prepare('SELECT * FROM persons WHERE qcc_url = ?').get(personUrl) as any;
  if (!person) throw new Error('Person not found');

  const personData = JSON.parse(person.raw_json);

  // Get all affiliated companies with details
  const affiliations: any[] = [];
  for (const company of personData.companies || []) {
    const companyRecord = db.prepare('SELECT * FROM companies WHERE qcc_url = ?').get(company.profileUrl) as any;
    if (companyRecord) {
      const companyData = JSON.parse(companyRecord.raw_json);
      // Find this person in the company's directors
      const directorEntry = companyData.directors?.find((d: any) =>
        d.name === person.name || d.profileUrl === personUrl
      );
      affiliations.push({
        companyName: companyData.companyName,
        position: directorEntry?.position || company.position || 'Unknown',
        registeredCapital: companyData.registeredCapital,
        establishedDate: companyData.establishedDate,
        operatingStatus: companyData.operatingStatus
      });
    }
  }

  // Sort by established date
  affiliations.sort((a, b) => {
    if (!a.establishedDate) return 1;
    if (!b.establishedDate) return -1;
    return a.establishedDate.localeCompare(b.establishedDate);
  });

  // Create document
  const doc = new Document({
    sections: [{
      properties: {},
      children: [
        new Paragraph({
          text: `Due Diligence Report: ${person.name}`,
          heading: HeadingLevel.HEADING_1
        }),
        new Paragraph({
          text: `Generated: ${new Date().toISOString().split('T')[0]}`
        }),
        new Paragraph({
          text: `Source: ${personUrl}`
        }),
        new Paragraph({ text: '' }),
        new Paragraph({
          text: 'Company Affiliations',
          heading: HeadingLevel.HEADING_2
        }),
        new Paragraph({
          text: `Total: ${affiliations.length} companies`
        }),
        new Paragraph({ text: '' }),
        new Table({
          rows: [
            new TableRow({
              children: [
                new TableCell({ children: [new Paragraph('Company')] }),
                new TableCell({ children: [new Paragraph('Position')] }),
                new TableCell({ children: [new Paragraph('Capital')] }),
                new TableCell({ children: [new Paragraph('Established')] }),
                new TableCell({ children: [new Paragraph('Status')] }),
              ]
            }),
            ...affiliations.map(a => new TableRow({
              children: [
                new TableCell({ children: [new Paragraph(a.companyName || '')] }),
                new TableCell({ children: [new Paragraph(a.position || '')] }),
                new TableCell({ children: [new Paragraph(a.registeredCapital || '')] }),
                new TableCell({ children: [new Paragraph(a.establishedDate || '')] }),
                new TableCell({ children: [new Paragraph(a.operatingStatus || '')] }),
              ]
            }))
          ]
        })
      ]
    }]
  });

  return await Packer.toBuffer(doc);
}

export async function saveReport(buffer: Buffer, filename: string) {
  const outputPath = `/Users/home/Desktop/DD Owl/reports/${filename}`;
  fs.mkdirSync('/Users/home/Desktop/DD Owl/reports', { recursive: true });
  fs.writeFileSync(outputPath, buffer);
  return outputPath;
}
```

**Step 3: Add report endpoint to server**

```typescript
import { generatePersonReport, saveReport } from './report';

app.get('/api/report/person', async (req, res) => {
  try {
    const url = req.query.url as string;
    if (!url) {
      return res.status(400).json({ error: 'URL required' });
    }

    const buffer = await generatePersonReport(url);
    const filename = `person-report-${Date.now()}.docx`;
    const path = await saveReport(buffer, filename);

    res.json({ success: true, path, filename });
  } catch (error) {
    res.status(500).json({ error: error.message });
  }
});
```

---

## Task 9: Add Generate Report Button to Extension

**Files:**
- Modify: `owl1-extension/popup/popup.html`
- Modify: `owl1-extension/popup/popup.js`

**Step 1: Add report button to popup**

```html
<button id="generateReportBtn" class="secondary" style="display: none;">Generate Report</button>
```

**Step 2: Handle report generation**

```javascript
const generateReportBtn = document.getElementById('generateReportBtn');

// Show report button after extraction
// In the extract success handler:
if (response.data.pageType === 'person_profile') {
  generateReportBtn.style.display = 'block';
  generateReportBtn.dataset.url = response.data.sourceUrl;
}

generateReportBtn.addEventListener('click', async () => {
  const url = generateReportBtn.dataset.url;
  generateReportBtn.textContent = 'Generating...';
  generateReportBtn.disabled = true;

  try {
    const response = await fetch(`http://localhost:8080/api/report/person?url=${encodeURIComponent(url)}`);
    const result = await response.json();

    if (result.success) {
      generateReportBtn.textContent = 'Report saved!';
      resultEl.textContent = `Report saved to: ${result.path}`;
      resultEl.classList.add('visible');
    } else {
      generateReportBtn.textContent = 'Failed';
    }
  } catch (error) {
    generateReportBtn.textContent = 'Error';
  }

  setTimeout(() => {
    generateReportBtn.textContent = 'Generate Report';
    generateReportBtn.disabled = false;
  }, 3000);
});
```

---

## Verification Checklist

- [ ] SQLite database creates and stores data
- [ ] Extension extraction saves to database
- [ ] Linked profiles are queued automatically
- [ ] Pagination is detected in extraction response
- [ ] Click-next-page works via extension message
- [ ] Puppeteer connects to existing Chrome
- [ ] Auto-mode crawls through queue
- [ ] Report generates Word document with affiliations table
- [ ] Full workflow: Extract person → auto-crawl companies → generate report

---

## Usage

**Manual Mode:**
1. Navigate to QCC person profile
2. Click Extract
3. Repeat for each affiliated company
4. Click Generate Report

**Auto Mode:**
1. Start Chrome with: `open -a "Google Chrome" --args --remote-debugging-port=9222`
2. Navigate to QCC person profile
3. Click Extract (queues all affiliations)
4. Click Start Auto Crawl
5. Wait for queue to complete
6. Click Generate Report
